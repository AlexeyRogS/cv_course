{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexeyRogS/cv_course/blob/week6/week6/vit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1USYB3UvvsF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ViT"
      ],
      "metadata": {
        "id": "pqb_7_SD0xnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchPartitioner(nn.Module):\n",
        "    def __init__(self, in_h, in_w, out_h, out_w, in_channels, embedding_dim):\n",
        "        super(PatchPartitioner, self).__init__()\n",
        "        assert in_h % out_h == 0 and in_w % out_w == 0 and in_h // out_h == in_w // out_w\n",
        "        k_size = in_h // out_h\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        self.conv = nn.Conv2d(in_channels, embedding_dim, k_size, stride=k_size)\n",
        "\n",
        "        self.flatten = nn.Flatten(2, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.flatten(self.conv(x)).transpose(1, 2)"
      ],
      "metadata": {
        "id": "67K6qy1Zv_2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pp = PatchPartitioner(in_h=32, in_w=32, out_h=16, out_w=16, in_channels=1, embedding_dim=100)\n",
        "result = pp(torch.zeros((1, 1, 32, 32)))\n",
        "assert result.shape == (1, 16*16, 100)"
      ],
      "metadata": {
        "id": "Px9BemZ7xeCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MSA (multi head self-attention)"
      ],
      "metadata": {
        "id": "D0FmIoWo0zKA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://i.ibb.co/1q04DSF/Screenshot-151.png\" width='300' height='600'>"
      ],
      "metadata": {
        "id": "TN4rJEW_6yl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.nn.MultiheadAttention?"
      ],
      "metadata": {
        "id": "X_MnvbGhxuOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP"
      ],
      "metadata": {
        "id": "c6Mjt8SV099P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mlp(embedding_dim, hidden_dim, dropout_rate):\n",
        "    return nn.Sequential(\n",
        "        # YOUR CODE HERE: Liner + GELU + Dropout + Linear + Dropout\n",
        "        nn.Linear(embedding_dim, hidden_dim),\n",
        "        nn.GELU(),\n",
        "        nn.Dropout(dropout_rate),\n",
        "        nn.Linear(hidden_dim, embedding_dim),\n",
        "        nn.Dropout(dropout_rate),\n",
        "    )\n",
        "mlp = get_mlp(100, 100*2, 0.1)"
      ],
      "metadata": {
        "id": "e3Pa0dsBzGcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn.LayerNorm?"
      ],
      "metadata": {
        "id": "CtagremlzGe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Стохастическая глубина (stochastic depth)"
      ],
      "metadata": {
        "id": "ZVqDEI9Q0nb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DropPath(nn.Module):\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        if not self.training:\n",
        "            return x\n",
        "        # YOUR CODE HERE: generate random tenzor, create mask from it and multiply x by the mask. also divide result by 1 - drop_prob\n",
        "        shape = (x.shape[0],) + (1,)*(x.ndim - 1)\n",
        "        mask = (torch.rand(shape) > self.drop_prob).type(x.dtype)\n",
        "        return x * mask / (1 - self.drop_prob)"
      ],
      "metadata": {
        "id": "VRE--cotzGiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Собираем блок энкодера"
      ],
      "metadata": {
        "id": "Cgcn5Z0r2z82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_heads, mlp_hidden_dim, dropout=0.1, attention_dropout=0.1, drop_path_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.attention_norm = nn.LayerNorm(embedding_dim)\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads, dropout=attention_dropout, batch_first=True)\n",
        "        self.attention_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.mlp_norm = nn.LayerNorm(embedding_dim)\n",
        "        self.mlp = get_mlp(embedding_dim, mlp_hidden_dim, dropout)\n",
        "        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_prime = self.attention_norm(x)\n",
        "        x_prime = self.attention(x_prime, x_prime, x_prime)[0]\n",
        "        x_prime = self.attention_dropout(x_prime)\n",
        "        x = x + self.drop_path(x_prime)\n",
        "\n",
        "        x_prime = self.mlp_norm(x)\n",
        "        x_prime = self.mlp(x_prime)\n",
        "        x = x + self.drop_path(x_prime)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Dm-UsjDE2dBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywXKFhA75L03",
        "outputId": "60b0243b-26a9-4c74-ad9a-c38eaa551c25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 256, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc = TransformerEncoder(embedding_dim=100, num_heads=10, mlp_hidden_dim=200)\n",
        "assert enc(result).shape == result.shape"
      ],
      "metadata": {
        "id": "EdGf2cJd8HW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Позиционные эмбеддинги"
      ],
      "metadata": {
        "id": "E_DhgJmW8kCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_patches = 16 * 16\n",
        "embedding_dim = 64\n",
        "\n",
        "# YOUR CODE HERE\n",
        "emb = torch.nn.Parameter(torch.empty((n_patches, embedding_dim)))\n",
        "\n",
        "torch.nn.init.trunc_normal_(emb, std=0.2);"
      ],
      "metadata": {
        "id": "zm9u1B0s8afD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class emb"
      ],
      "metadata": {
        "id": "5-6Wh_vH92Sz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_emb = torch.nn.Parameter(torch.empty((1, embedding_dim)))\n",
        "torch.nn.init.trunc_normal_(emb, std=0.2);"
      ],
      "metadata": {
        "id": "cMuvDFPn9rng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Собираем все вместе в ViT"
      ],
      "metadata": {
        "id": "oQ7T2nkmUT_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(nn.Module):\n",
        "    def __init__(self, in_h, in_w, n_patches, in_channels, embedding_dim,\n",
        "                 num_layers, num_heads, mlp_hidden_dim, num_classes=1000,\n",
        "                 dropout=0.1, attention_dropout=0.1, depth_dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.pp = PatchPartitioner(in_h, in_w,\n",
        "                                   int(n_patches**0.5), int(n_patches**0.5),\n",
        "                                   in_channels, embedding_dim\n",
        "                                   )\n",
        "        self.pos_embeddings = torch.nn.Parameter(torch.empty((1, n_patches, embedding_dim)))\n",
        "        torch.nn.init.trunc_normal_(self.pos_embeddings, std=0.2)\n",
        "\n",
        "        self.class_embedding = torch.nn.Parameter(torch.empty((1, 1, embedding_dim)))\n",
        "        torch.nn.init.trunc_normal_(self.class_embedding, std=0.2)\n",
        "\n",
        "        depth_dropout_rates = [x.item() for x in torch.linspace(0, depth_dropout, num_layers)]\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            TransformerEncoder(embedding_dim, num_heads, mlp_hidden_dim, dropout,\n",
        "                               attention_dropout, drop_path_rate)\n",
        "            for drop_path_rate in depth_dropout_rates\n",
        "        ])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.norm = nn.LayerNorm(embedding_dim)\n",
        "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        patches = self.pp(x)\n",
        "        patches = patches + self.pos_embeddings\n",
        "\n",
        "        x = torch.cat((self.class_embedding.expand(patches.shape[0], 1, -1), patches), dim=1)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        return self.fc(x[:,0])"
      ],
      "metadata": {
        "id": "JpPexMp6HCMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = torch.rand((16, 3, 224, 224))\n",
        "vit = ViT(224, 224, 256, 3, 64, 6, 8, 128)\n",
        "vit(batch).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZWN-W1GXaJ4",
        "outputId": "a6e71396-45cd-41f0-d19a-4beb58b03671"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 1000])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Трюки для обучения"
      ],
      "metadata": {
        "id": "CktC7LZuZkIC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Warm-up и расписание"
      ],
      "metadata": {
        "id": "_rBo3ov-ZmhK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Аугментация данных\n",
        "\n",
        "* cutmix\n",
        "* cutout\n",
        "* mixup"
      ],
      "metadata": {
        "id": "yH670bQwZsvl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2W7xPIscZlub"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}